from langchain.text_splitter import TextSplitter
from langchain_core.documents import Document
from pythainlp.tokenize import word_tokenize

class CustomThaiTextSplitter(TextSplitter):
    def __init__(self, chunk_size=1000, chunk_overlap=100):
        super().__init__()
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def split_text(self, text):
        words = word_tokenize(text, keep_whitespace=False)  # à¹ƒà¸Šà¹‰ pythainlp tokenizer
        chunks = []
        start = 0

        while start < len(words):
            end = start + self.chunk_size
            chunk = " ".join(words[start:end])
            chunks.append(chunk)
            start += self.chunk_size - self.chunk_overlap  # à¹ƒà¸Šà¹‰ overlap à¹€à¸žà¸·à¹ˆà¸­à¸¥à¸”à¸à¸²à¸£à¸ªà¸¹à¸à¹€à¸ªà¸µà¸¢à¸‚à¹‰à¸­à¸¡à¸¹à¸¥

        return chunks

# ðŸ”¹ à¹ƒà¸Šà¹‰à¸‡à¸²à¸™ Splitter à¸à¸±à¸š LangChain
text = "à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¸£à¸±à¸š à¸§à¸±à¸™à¸™à¸µà¹‰à¸­à¸²à¸à¸²à¸¨à¸”à¸µà¸¡à¸²à¸ à¸œà¸¡à¸­à¸¢à¸²à¸à¹„à¸›à¹€à¸—à¸µà¹ˆà¸¢à¸§à¸—à¸°à¹€à¸¥à¸à¸±à¸šà¹€à¸žà¸·à¹ˆà¸­à¸™à¹† à¹à¸•à¹ˆà¸•à¹‰à¸­à¸‡à¸—à¸³à¸‡à¸²à¸™à¸à¹ˆà¸­à¸™"

splitter = CustomThaiTextSplitter(chunk_size=10, chunk_overlap=2)
chunks = splitter.split_text(text)

for i, chunk in enumerate(chunks):
    print(f"Chunk {i+1}: {chunk}")
